{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4ba99fb1-9587-4108-8df5-d29c378bc807",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted Text:\n",
      "Certificate of Virtual Internship\n",
      "\n",
      "This isto certify that\n",
      "\n",
      "VISHAKHA SANJEEV SHINDE\n",
      "\n",
      "Intemational Institute of Information Technology (1A?'T)\n",
      "\n",
      "has successfully completed 10 week\n",
      "AML Virtual Internship\n",
      "During January - March 2024\n",
      "\n",
      "Supported By : India Edu Program\n",
      "\n",
      "Google for Developer\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import pytesseract\n",
    "import re\n",
    "\n",
    "# Set up the Tesseract executable path (adjust the path if necessary)\n",
    "pytesseract.pytesseract.tesseract_cmd = r'C:\\Users\\Admin\\AppData\\Local\\Programs\\TesseractOCR\\tesseract.exe'\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Load the image\n",
    "image_path = 'sample1.jpeg'\n",
    "image = cv2.imread(image_path)\n",
    "\n",
    "# Convert the image to grayscale\n",
    "gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# Apply sharpening filter\n",
    "kernel = np.array([[0, -1, 0], \n",
    "                   [-1, 5,-1], \n",
    "                   [0, -1, 0]])\n",
    "sharpened = cv2.filter2D(gray, -1, kernel)\n",
    "\n",
    "# Apply edge detection\n",
    "edges = cv2.Canny(sharpened, 50, 150, apertureSize=3)\n",
    "\n",
    "\n",
    "# # Apply edge detection\n",
    "# edges = cv2.Canny(gray, 50, 150, apertureSize=3)\n",
    "\n",
    "# Find contours\n",
    "contours, _ = cv2.findContours(edges, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "# Sort contours by area (largest to smallest) and take the largest one\n",
    "contours = sorted(contours, key=cv2.contourArea, reverse=True)[:1]\n",
    "\n",
    "for contour in contours:\n",
    "    # Get the bounding box of the largest contour\n",
    "    x, y, w, h = cv2.boundingRect(contour)\n",
    "    # Crop the image to the bounding box\n",
    "    cropped_image = image[y:y+h, x:x+w]\n",
    "\n",
    "# Convert the cropped image to PIL format for OCR\n",
    "cropped_image_pil = Image.fromarray(cv2.cvtColor(cropped_image, cv2.COLOR_BGR2RGB))\n",
    "\n",
    "# Perform OCR\n",
    "text = pytesseract.image_to_string(cropped_image_pil)\n",
    "\n",
    "print(\"Extracted Text:\")\n",
    "print(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e6573f5e-fe10-4993-8231-2ac0072a3afd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "institute: IA#IT\n",
      "dates: January - March\n",
      "issued_by: Technology (IA#IT)\n"
     ]
    }
   ],
   "source": [
    "# Define dictionaries to store labels and regex patterns\n",
    "entity_labels = {\n",
    "  \"name\": r'\\b[A-Z][a-zA-Z\\'-]\\s[A-Z][a-zA-Z\\'-]\\b(?:\\s[A-Z][a-zA-Z\\'-]*)?',\n",
    "  \"institute\": r\"\\(([^)]+)\\)\",  # Capture institute name only\n",
    "  \"internship\": r\"\\d+ weeks of [A-Za-z\\s\\-]+Internship\",\n",
    "  \"dates\": r\"During\\s+(.+?)\\s+20\\d{2}\",\n",
    "  \"organizer\": r\"(?:By|Supported By):\\s+(.+)\",\n",
    "  \"issued_by\": r\"([A-Z][a-z]+\\s+\\(.*\\))\",  # Capture everything within parentheses\n",
    "  \"approved_by\": r\"Dr\\.\\s+([A-Z][a-z]+)\\s+\\(.*\\)\"  # Capture everything within parentheses\n",
    "}\n",
    "\n",
    "# Extract entities\n",
    "extracted_entities = {}\n",
    "for label, pattern in entity_labels.items():\n",
    "  match = re.search(pattern, text, re.MULTILINE)  # Use re.MULTILINE for multi-line text\n",
    "  if match:\n",
    "    try:\n",
    "      extracted_entities[label] = match.group(1)  # Capture group 1 (extracted text)\n",
    "    except IndexError:  # Handle cases where the group doesn't exist\n",
    "      # Optional: Provide informative message or default value\n",
    "      print(f\"Warning: No group 1 captured for label '{label}'.\")\n",
    "      extracted_entities[label] = None  # Or set a default value\n",
    "\n",
    "# Print extracted entities with labels\n",
    "for label, entity in extracted_entities.items():\n",
    "  print(f\"{label}: {entity}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a2ab2218-15ab-4975-8264-92ab2d0809e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Example text\n",
    "# text = \"Acme Corporation Ltd, Globex Inc. Ltd, Stark Industries Pvt Ltd, Stark Industries Offered by Stark Industries\"\n",
    "\n",
    "# Regex pattern to match company name with various keywords\n",
    "regex = r'(\\w+(?:\\s\\w+)*)\\s(?:Inc\\.\\s)?(?:Pvt\\s)?(?:Offered\\sby\\s)?Ltd'\n",
    "\n",
    "# Find all matches in the text\n",
    "matches = re.findall(regex, text, re.IGNORECASE)\n",
    "\n",
    "# Output the company names\n",
    "for company in matches:\n",
    "    print(company)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0e2c2424-64aa-4c74-988e-3ecdd8158597",
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = r'Dr\\.\\s+([A-Z][a-z]+(?:\\s[A-Z][a-z]+)*)'\n",
    "pattern1 = r'(\\b\\w+(?:\\s\\w+)*)\\s(?:Inc\\.\\s)?(?:Pvt\\s(?:Ltd)?\\b)?(?:Offered\\sby\\s)?(?:Ltd|Corp\\b)'\n",
    "\n",
    "# Find all matches\n",
    "matches = re.findall(pattern, text)\n",
    "\n",
    "# Print the matches\n",
    "for match in matches:\n",
    "    print(f\"Found: {match}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dc31b6fb-8525-4d67-9abf-6a96a22fbbfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regex pattern to match date range\n",
    "date_pattern = r'period of\\s+(\\d{1,2} [A-Za-z]+ \\d{4})\\s*-\\s*(\\d{1,2} [A-Za-z]+ \\d{4})'\n",
    "\n",
    "# Find all matches in the text\n",
    "matches = re.findall(date_pattern, text)\n",
    "\n",
    "# Output the results\n",
    "for match in matches:\n",
    "    start_date, end_date = match\n",
    "    print(f\"Date range: From {start_date} to {end_date}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "28377ff2-cec7-415f-9863-27fd95a39694",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 weeks\n"
     ]
    }
   ],
   "source": [
    "# Regex pattern to match durations\n",
    "regex = r'\\b(?:(\\d+)\\s*weeks|(\\d+)\\s*months|tenure\\s+of\\s+(\\d+)|of\\s+(\\d+))\\b'\n",
    "\n",
    "# Find all matches in the text\n",
    "matches = re.findall(regex, text)\n",
    "\n",
    "# Output the matches\n",
    "for match in matches:\n",
    "    if match[0]:\n",
    "        print(f\"{match[0]} weeks\")\n",
    "    elif match[1]:\n",
    "        print(f\"{match[1]} months\")\n",
    "    elif match[2]:\n",
    "        print(f\"tenure of {match[2]} months\")\n",
    "    else:\n",
    "        print(f\"of {match[3]} months\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b6eb7f37-c655-4041-9fd5-7a28991160a7",
   "metadata": {},
   "outputs": [],
   "source": [
    " def extract_name(text):\n",
    "    \"\"\"Function to extract name from resume text using spaCy's Matcher.\"\"\"\n",
    "    nlp = spacy.load('en_core_web_sm')  # Load English language model\n",
    "    matcher = Matcher(nlp.vocab)  # Initialize Matcher with spaCy's vocabulary\n",
    "\n",
    "    # Define name patterns for Matcher\n",
    "    patterns = [\n",
    "        [{'POS': 'PROPN'}, {'POS': 'PROPN'}],  # First name and Last name\n",
    "        [{'POS': 'PROPN'}, {'POS': 'PROPN'}, {'POS': 'PROPN'}],  # First name, Middle name, and Last name\n",
    "        [{'POS': 'PROPN'}, {'POS': 'PROPN'}, {'POS': 'PROPN'}, {'POS': 'PROPN'}]  # First name, Middle name, Middle name, and Last name\n",
    "        # Add more patterns as needed\n",
    "    ]\n",
    "\n",
    "    for pattern in patterns:\n",
    "        matcher.add('NAME', patterns=[pattern])  # Add each pattern to the Matcher\n",
    "\n",
    "    doc = nlp(text)  # Process the resume text with spaCy\n",
    "    matches = matcher(doc)  # Apply the Matcher to find name patterns\n",
    "\n",
    "    for match_id, start, end in matches:\n",
    "        span = doc[start:end]  # Get the matched span of text\n",
    "        return span.text  # Return the matched text (name)\n",
    "\n",
    "    return None  # Return None if no name is found\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7fe1eb8c-bef0-41f3-88e5-901ca5dedae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_proper_nouns(text):\n",
    "    \"\"\"Function to extract proper nouns from text using SpaCy.\"\"\"\n",
    "    nlp = spacy.load(\"en_core_web_sm\")  # Load SpaCy model\n",
    "    doc = nlp(text)  # Process the text with SpaCy\n",
    "\n",
    "    # Extract proper nouns\n",
    "    proper_nouns = [token.text for token in doc if token.pos_ == \"PROPN\"]\n",
    "    return proper_nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1d9f9832-5d58-444c-9552-867b901095fd",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'spacy' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Extract name\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m name \u001b[38;5;241m=\u001b[39m \u001b[43mextract_name\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExtracted Name:\u001b[39m\u001b[38;5;124m\"\u001b[39m, name)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Extract proper nouns\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[11], line 3\u001b[0m, in \u001b[0;36mextract_name\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mextract_name\u001b[39m(text):\n\u001b[0;32m      2\u001b[0m \u001b[38;5;250m   \u001b[39m\u001b[38;5;124;03m\"\"\"Function to extract name from resume text using spaCy's Matcher.\"\"\"\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m    nlp \u001b[38;5;241m=\u001b[39m \u001b[43mspacy\u001b[49m\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124men_core_web_sm\u001b[39m\u001b[38;5;124m'\u001b[39m)  \u001b[38;5;66;03m# Load English language model\u001b[39;00m\n\u001b[0;32m      4\u001b[0m    matcher \u001b[38;5;241m=\u001b[39m Matcher(nlp\u001b[38;5;241m.\u001b[39mvocab)  \u001b[38;5;66;03m# Initialize Matcher with spaCy's vocabulary\u001b[39;00m\n\u001b[0;32m      6\u001b[0m    \u001b[38;5;66;03m# Define name patterns for Matcher\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'spacy' is not defined"
     ]
    }
   ],
   "source": [
    "# Extract name\n",
    "name = extract_name(text)\n",
    "print(\"Extracted Name:\", name)\n",
    "\n",
    "# Extract proper nouns\n",
    "proper_nouns = extract_proper_nouns(text)\n",
    "print(\"Proper Nouns:\", proper_nouns)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
